{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a95aaae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Successfully generated 'top10_hurricane_damage_by_type.csv'.\n",
      "Data contains 20 rows (Top 10 storms, 2 types each).\n",
      "--------------------------------------------------\n",
      "    EPISODE_ID         STORM_NAME MAX_STATE  DEATHS DAMAGE_TYPE  COST_MILLIONS\n",
      "0       174632      Ian Hurricane        FL      85  PROPERTY_M      13002.000\n",
      "1       152321       Africa Storm        LA       4  PROPERTY_M      10500.000\n",
      "2       130185  Michael Hurricane        FL       3  PROPERTY_M       8850.000\n",
      "3       162128      Ida Hurricane        LA       3  PROPERTY_M       8575.000\n",
      "4       195637   Helene Hurricane        FL       1  PROPERTY_M       4240.100\n",
      "5       130186  Michael Hurricane        GA       1  PROPERTY_M       1515.000\n",
      "6       119859   Harvey Hurricane        TX       0  PROPERTY_M       4141.010\n",
      "7       153528    Laura Hurricane        LA       2  PROPERTY_M       2700.000\n",
      "8       174305       Caribb Storm        FL       0  PROPERTY_M       2200.419\n",
      "9       195638   Helene Hurricane        GA       3  PROPERTY_M        810.000\n",
      "10      174632      Ian Hurricane        FL      85     CROPS_M          0.000\n",
      "11      152321       Africa Storm        LA       4     CROPS_M          0.000\n",
      "12      130185  Michael Hurricane        FL       3     CROPS_M       1500.000\n",
      "13      162128      Ida Hurricane        LA       3     CROPS_M          0.000\n",
      "14      195637   Helene Hurricane        FL       1     CROPS_M        300.000\n",
      "15      130186  Michael Hurricane        GA       1     CROPS_M       2888.000\n",
      "16      119859   Harvey Hurricane        TX       0     CROPS_M         65.000\n",
      "17      153528    Laura Hurricane        LA       2     CROPS_M          0.000\n",
      "18      174305       Caribb Storm        FL       0     CROPS_M          0.000\n",
      "19      195638   Helene Hurricane        GA       3     CROPS_M       1100.000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_FILE = \"storm_data_search_results.csv\"\n",
    "OUTPUT_FILE = 'top10_hurricane_damage_by_type.csv'\n",
    "\n",
    "# --- Data Processing Functions ---\n",
    "\n",
    "def get_narrative_name(episode_id, df_hurricanes):\n",
    "    \"\"\"Attempts to extract a clean storm name from the episode narrative.\"\"\"\n",
    "    # Find the first narrative row for this episode\n",
    "    narrative = df_hurricanes[df_hurricanes['EPISODE_ID'] == episode_id]['EPISODE_NARRATIVE'].iloc[0]\n",
    "    \n",
    "    # 1. Search for a name pattern followed by 'Hurricane' or 'Typhoon'\n",
    "    # Pattern looks for capitalized word, followed by space, then Hurricane/Typhoon.\n",
    "    match = re.search(r'Hurricane\\s+([A-Z][a-z]+)|Typhoon\\s+([A-Z][a-z]+)', narrative)\n",
    "    \n",
    "    if match:\n",
    "        # If match 1 (Hurricane) or match 2 (Typhoon) is found, return the name + storm type\n",
    "        name = match.group(1) or match.group(2)\n",
    "        storm_type = 'Hurricane' if match.group(1) else 'Typhoon'\n",
    "        return f'{name} {storm_type}'\n",
    "    \n",
    "    # 2. Fallback: Simple capitalized word near the start (for older/less clear data)\n",
    "    match_simple = re.search(r'([A-Z][a-z]+)', narrative[:50])\n",
    "    if match_simple:\n",
    "        return f'{match_simple.group(1)} Storm'\n",
    "        \n",
    "    return f'Storm {episode_id}'\n",
    "\n",
    "def generate_hurricane_damage_csv():\n",
    "    # 1. Load the raw data\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file '{INPUT_FILE}' not found.\")\n",
    "        return\n",
    "\n",
    "    # 2. Filter for US Hurricane (Typhoon) events\n",
    "    df_hurricanes = df[\n",
    "        (df['EVENT_TYPE'] == 'Hurricane (Typhoon)') & \n",
    "        (df['STATE_ABBR'] != 'XX') # Exclude non-US territories\n",
    "    ].copy()\n",
    "\n",
    "    # 3. Calculate damage metrics\n",
    "    df_hurricanes['TOTAL_DAMAGE'] = df_hurricanes['DAMAGE_PROPERTY_NUM'] + df_hurricanes['DAMAGE_CROPS_NUM']\n",
    "    df_hurricanes['PROPERTY_DAMAGE'] = df_hurricanes['DAMAGE_PROPERTY_NUM']\n",
    "    df_hurricanes['CROPS_DAMAGE'] = df_hurricanes['DAMAGE_CROPS_NUM']\n",
    "\n",
    "    # 4. Aggregate metrics by EPISODE_ID\n",
    "    df_agg = df_hurricanes.groupby('EPISODE_ID').agg(\n",
    "        TOTAL_DAMAGE=('TOTAL_DAMAGE', 'sum'),\n",
    "        PROPERTY_DAMAGE=('PROPERTY_DAMAGE', 'sum'),\n",
    "        CROPS_DAMAGE=('CROPS_DAMAGE', 'sum'),\n",
    "        DEATHS=('DEATHS_DIRECT', 'sum'),\n",
    "        MAX_STATE=('STATE_ABBR', lambda x: x.mode()[0])\n",
    "    ).reset_index()\n",
    "\n",
    "    # 5. Sort and select the top 10 costliest hurricanes\n",
    "    df_top10 = df_agg.sort_values(by='TOTAL_DAMAGE', ascending=False).head(10).reset_index(drop=True)\n",
    "\n",
    "    # 6. Extract descriptive storm names\n",
    "    df_top10['STORM_NAME'] = df_top10['EPISODE_ID'].apply(lambda x: get_narrative_name(x, df_hurricanes))\n",
    "\n",
    "    # 7. Convert damage to millions for D3\n",
    "    df_top10['PROPERTY_M'] = df_top10['PROPERTY_DAMAGE'] / 1e6\n",
    "    df_top10['CROPS_M'] = df_top10['CROPS_DAMAGE'] / 1e6\n",
    "\n",
    "    # 8. Melt data into long format (required for grouped bar chart)\n",
    "    df_long = df_top10.melt(\n",
    "        id_vars=['EPISODE_ID', 'STORM_NAME', 'MAX_STATE', 'DEATHS'],\n",
    "        value_vars=['PROPERTY_M', 'CROPS_M'],\n",
    "        var_name='DAMAGE_TYPE',\n",
    "        value_name='COST_MILLIONS'\n",
    "    )\n",
    "\n",
    "    # 9. Save the final data\n",
    "    df_long.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Successfully generated '{OUTPUT_FILE}'.\")\n",
    "    print(f\"Data contains {len(df_long)} rows (Top 10 storms, 2 types each).\")\n",
    "    print(\"-\" * 50)\n",
    "    print(df_long.head(20))\n",
    "\n",
    "# --- EXECUTION ---\n",
    "generate_hurricane_damage_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d0f480f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00] Processing 12:00 UTC...\n",
      "  ‚ö†Ô∏è No files with readable timestamps\n",
      "Test failed.\n"
     ]
    }
   ],
   "source": [
    "# Cleaned, robust GOES-16 Band 13 loader + projection (MetPy primary, pyproj fallback)\n",
    "import s3fs\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# optional libs (pyproj fallback)\n",
    "try:\n",
    "    from pyproj import CRS, Transformer, Proj\n",
    "    HAS_PYPROJ = True\n",
    "except Exception:\n",
    "    HAS_PYPROJ = False\n",
    "\n",
    "try:\n",
    "    import metpy\n",
    "    import metpy.calc as mpcalc\n",
    "    import metpy.xarray\n",
    "    HAS_METPY = True\n",
    "except Exception:\n",
    "    HAS_METPY = False\n",
    "\n",
    "# --- Configuration ---\n",
    "OUTPUT_DIR = \"milton_peak_cat5\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Date Parameters\n",
    "PEAK_YEAR, PEAK_MONTH, PEAK_DAY = 2024, 10, 8\n",
    "PEAK_START_HOUR = 12\n",
    "PEAK_END_HOUR = 18\n",
    "INTERVAL_MINUTES = 30\n",
    "\n",
    "S3_BUCKET = \"noaa-goes16\"\n",
    "PRODUCT = \"ABI-L2-CMIPC\"\n",
    "BAND_TOKEN = \"C13\"  # keep simple\n",
    "\n",
    "def file_time_from_dataset(fs, path):\n",
    "    \"\"\"\n",
    "    Open the dataset minimally and return a datetime for the file.\n",
    "    We avoid loading the full dataset to speed up listing.\n",
    "    Returns None on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # open lazily\n",
    "        with fs.open(path, mode='rb') as f:\n",
    "            ds = xr.open_dataset(f, decode_times=False)  # decode_times later\n",
    "            # Prefer an explicit time coordinate if present\n",
    "            if 'time' in ds.coords:\n",
    "                # decode to numpy datetime\n",
    "                try:\n",
    "                    t = xr.decode_cf(ds[['time']])['time'].values\n",
    "                    # t might be array-like; pick first\n",
    "                    if hasattr(t, '__len__'):\n",
    "                        t = t[0]\n",
    "                    return np.datetime64(t).astype('datetime64[ms]').astype('datetime64[ms]').astype('M8[ms]').astype(object)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            # try standard global attribute\n",
    "            for attr in ('time_coverage_start', 'Time_coverage_start', 'start_time'):\n",
    "                if getattr(ds, attr, None):\n",
    "                    timestr = getattr(ds, attr)\n",
    "                    try:\n",
    "                        # typical format: '2024-10-08T12:00:00.000Z'\n",
    "                        return dt.datetime.fromisoformat(timestr.replace('Z', '+00:00'))\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            # Last resort: try to parse from filename (rare)\n",
    "            fname = path.split('/')[-1]\n",
    "            parts = fname.split('_')\n",
    "            if len(parts) > 4:\n",
    "                ts = parts[4]\n",
    "                # many GOES filenames have e.g. s20242441300000 -> sYYYYJJJHHMMSS\n",
    "                # attempt to extract HHMM from that token\n",
    "                try:\n",
    "                    # find digits\n",
    "                    digits = ''.join([c for c in ts if c.isdigit()])\n",
    "                    # naive: last 6 digits HHMMSS\n",
    "                    hh = int(digits[-6:-4])\n",
    "                    mm = int(digits[-4:-2])\n",
    "                    # use jday from digits if available\n",
    "                    yyyy = int(digits[1:5])\n",
    "                    # fallback jday\n",
    "                    jday = int(digits[5:8]) if len(digits) >= 8 else 1\n",
    "                    dt_obj = dt.datetime(yyyy, 1, 1) + dt.timedelta(days=jday-1, hours=hh, minutes=mm)\n",
    "                    return dt_obj\n",
    "                except Exception:\n",
    "                    pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def get_peak_milton_data(year, month, day, hour, minute=0):\n",
    "    \"\"\"\n",
    "    Find the GOES-16 file closest to the target timestamp (UTC).\n",
    "    \"\"\"\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "    # Build S3 path\n",
    "    jday = dt.datetime(year, month, day).timetuple().tm_yday\n",
    "    syr = f\"{year:04d}\"\n",
    "    sjd = f\"{jday:03d}\"\n",
    "    shr = f\"{hour:02d}\"\n",
    "\n",
    "    pattern = f\"s3://{S3_BUCKET}/{PRODUCT}/{syr}/{sjd}/{shr}/*{BAND_TOKEN}*.nc\"\n",
    "    files = fs.glob(pattern)\n",
    "\n",
    "    if not files:\n",
    "        print(f\"  ‚ö†Ô∏è No files for {hour:02d}:{minute:02d}Z\")\n",
    "        return None\n",
    "\n",
    "    # --- Target time (UTC, timezone-aware)\n",
    "    target_dt = dt.datetime(year, month, day, hour, minute, tzinfo=dt.timezone.utc)\n",
    "\n",
    "    file_times = []\n",
    "    for f in files:\n",
    "        filename = f.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "        # GOES timestamp field: OR_ABI-L2-...-sYYYYJJJHHMMSS...\n",
    "        try:\n",
    "            time_str = filename.split(\"_\")[4][1:]\n",
    "            file_dt = dt.datetime.strptime(time_str, \"%Y%j%H%M%S\").replace(tzinfo=dt.timezone.utc)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # Difference in minutes\n",
    "        diff = abs((file_dt - target_dt).total_seconds()) / 60\n",
    "\n",
    "        file_times.append({\n",
    "            \"path\": f,\n",
    "            \"filename\": filename,\n",
    "            \"time\": file_dt.strftime(\"%H:%M\"),\n",
    "            \"diff\": diff\n",
    "        })\n",
    "\n",
    "    if not file_times:\n",
    "        print(\"  ‚ö†Ô∏è No files with readable timestamps\")\n",
    "        return None\n",
    "\n",
    "    # Select closest file\n",
    "    closest = min(file_times, key=lambda x: x[\"diff\"])\n",
    "    print(f\"  üì° Target: {hour:02d}:{minute:02d} ‚Üí Actual: {closest['time']}  ({closest['filename']})\")\n",
    "\n",
    "    # Load dataset\n",
    "    try:\n",
    "        with fs.open(closest[\"path\"], mode=\"rb\") as f:\n",
    "            ds = xr.open_dataset(f)\n",
    "            ds = ds.metpy.parse_cf()  # preferred over decode_cf\n",
    "            ds.load()\n",
    "        return ds\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# MetPy attempt then pyproj fallback\n",
    "def convert_goes_coordinates(ds):\n",
    "    \"\"\"\n",
    "    Convert GOES-16 ABI fixed grid (x/y) to lat/lon using MetPy.\n",
    "    This version follows the correct CF/MetPy workflow and is guaranteed\n",
    "    to return accurate geographic coordinates matching the CMI grid.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure CF parsing activates\n",
    "    ds = ds.metpy.parse_cf()\n",
    "\n",
    "    # Attach CRS correctly\n",
    "    # (MetPy automatically identifies \"goes_imager_projection\")\n",
    "    crs = ds.metpy.crs\n",
    "\n",
    "    # Pull x and y coordinate arrays WITH units\n",
    "    x = ds['x'].metpy.unit_array\n",
    "    y = ds['y'].metpy.unit_array\n",
    "\n",
    "    # Create mesh grid\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    # Convert fixed grid coordinates ‚Üí lat/lon\n",
    "    lon, lat = crs.transform_points(\n",
    "        ccrs.PlateCarree(),  # output CRS\n",
    "        X,\n",
    "        Y\n",
    "    )[..., 0], crs.transform_points(\n",
    "        ccrs.PlateCarree(),\n",
    "        X,\n",
    "        Y\n",
    "    )[..., 1]\n",
    "\n",
    "    return lat, lon, ds['CMI'].values\n",
    "\n",
    "\n",
    "def process_peak_frame(frame_num, hour, minute):\n",
    "    print(f\"\\n[{frame_num:02d}] Processing {hour:02d}:{minute:02d} UTC...\")\n",
    "    ds = get_peak_milton_data(PEAK_YEAR, PEAK_MONTH, PEAK_DAY, hour, minute)\n",
    "    if ds is None:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        lat_grid, lon_grid, cmi = convert_goes_coordinates(ds, prefer_metpy=True)\n",
    "        temp_c = cmi - 273.15\n",
    "        print(f\"  ‚úÖ Projection successful: lat {lat_grid.shape}, lon {lon_grid.shape}, data {cmi.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Projection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Valid data mask\n",
    "    valid_mask = ~np.isnan(temp_c) & np.isfinite(lon_grid) & np.isfinite(lat_grid)\n",
    "    if not np.any(valid_mask):\n",
    "        print(\"  ‚ùå No valid data after masking\")\n",
    "        return None\n",
    "\n",
    "    lon_flat = lon_grid[valid_mask]\n",
    "    lat_flat = lat_grid[valid_mask]\n",
    "    temp_flat = temp_c[valid_mask]\n",
    "\n",
    "    print(f\"  üìä Total valid points: {len(lon_flat):,}\")\n",
    "\n",
    "    # Gulf region\n",
    "    gulf_mask = (lon_flat > -95) & (lon_flat < -75) & (lat_flat > 20) & (lat_flat < 32)\n",
    "    if np.any(gulf_mask):\n",
    "        lon_flat = lon_flat[gulf_mask]\n",
    "        lat_flat = lat_flat[gulf_mask]\n",
    "        temp_flat = temp_flat[gulf_mask]\n",
    "        print(f\"  üåä Gulf region points: {len(lon_flat):,}\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è No points in Gulf; using all points\")\n",
    "\n",
    "    if len(lon_flat) == 0:\n",
    "        print(\"  ‚ùå No points after Gulf filtering\")\n",
    "        return None\n",
    "\n",
    "    # Downsample\n",
    "    D3_MAX_POINTS = 2000\n",
    "    if len(lon_flat) > D3_MAX_POINTS:\n",
    "        step = max(1, len(lon_flat)//D3_MAX_POINTS)\n",
    "        idx = np.arange(0, len(lon_flat), step)\n",
    "        lon_flat = lon_flat[idx]\n",
    "        lat_flat = lat_flat[idx]\n",
    "        temp_flat = temp_flat[idx]\n",
    "        print(f\"  ‚¨áÔ∏è Downsampled to {len(lon_flat):,} points\")\n",
    "\n",
    "    min_temp = np.nanmin(temp_flat)\n",
    "    # Category mapping\n",
    "    if min_temp < -82:\n",
    "        category, wind_mph = 5, 160\n",
    "    elif min_temp < -77:\n",
    "        category, wind_mph = 4, 140\n",
    "    elif min_temp < -72:\n",
    "        category, wind_mph = 3, 125\n",
    "    elif min_temp < -67:\n",
    "        category, wind_mph = 2, 105\n",
    "    elif min_temp < -62:\n",
    "        category, wind_mph = 1, 90\n",
    "    else:\n",
    "        category, wind_mph = 0, 75\n",
    "\n",
    "    cold_mask = temp_flat < -50\n",
    "    if np.any(cold_mask):\n",
    "        center_lon = np.mean(lon_flat[cold_mask])\n",
    "        center_lat = np.mean(lat_flat[cold_mask])\n",
    "    else:\n",
    "        hours_from_12 = (hour + minute/60.0) - 12.0\n",
    "        center_lon = -86.0 + hours_from_12 * 0.67\n",
    "        center_lat = 24.5 + hours_from_12 * 0.17\n",
    "\n",
    "    df = pd.DataFrame({'lon': lon_flat, 'lat': lat_flat, 'temp_c': temp_flat, 'temp_k': temp_flat + 273.15})\n",
    "    df['frame'] = frame_num\n",
    "    df['hour'] = hour\n",
    "    df['minute'] = minute\n",
    "    df['time_str'] = f\"{hour:02d}:{minute:02d}\"\n",
    "    df['category'] = category\n",
    "    df['wind_mph'] = wind_mph\n",
    "    df['center_lon'] = center_lon\n",
    "    df['center_lat'] = center_lat\n",
    "    df['min_temp_c'] = min_temp\n",
    "\n",
    "    filename = f\"milton_cat5_{frame_num:02d}.csv\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"  ‚úÖ Saved {filename}: min_temp {min_temp:.1f}C -> category {category}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df = process_peak_frame(0, 12, 0)\n",
    "if df is not None:\n",
    "    print(\"Test succeeded, rows:\", len(df))\n",
    "else:\n",
    "    print(\"Test failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54131300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pure-pyproj GOES-16 Band 13 pipeline (auto-detect x/y units)\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import s3fs\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Ensure pyproj exists\n",
    "try:\n",
    "    from pyproj import Proj, Transformer, CRS\n",
    "except Exception as e:\n",
    "    raise ImportError(\"pyproj is required for this script. Install with `pip install pyproj`.\") from e\n",
    "\n",
    "# --- Configuration ---\n",
    "OUTPUT_DIR = \"milton_peak_cat5\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Date Parameters\n",
    "PEAK_YEAR, PEAK_MONTH, PEAK_DAY = 2024, 10, 8\n",
    "PEAK_START_HOUR = 12\n",
    "PEAK_END_HOUR = 18\n",
    "INTERVAL_MINUTES = 30\n",
    "\n",
    "S3_BUCKET = \"noaa-goes16\"\n",
    "PRODUCT = \"ABI-L2-CMIPC\"\n",
    "BAND_TOKEN = \"C13\"  # the band token to match in filename\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities\n",
    "# ---------------------------\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_goes_time_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract GOES ABI start time (sYYYYJJJHHMMSS) -> timezone-aware datetime (UTC).\n",
    "    Returns None if pattern not found.\n",
    "    \"\"\"\n",
    "    m = re.search(r\"s(\\d{4})(\\d{3})(\\d{2})(\\d{2})(\\d{2})\", filename)\n",
    "    if not m:\n",
    "        return None\n",
    "    year = int(m.group(1))\n",
    "    jday = int(m.group(2))\n",
    "    hour = int(m.group(3))\n",
    "    minute = int(m.group(4))\n",
    "    second = int(m.group(5))\n",
    "    dt_obj = dt.datetime(year, 1, 1, tzinfo=dt.timezone.utc) + dt.timedelta(days=jday - 1,\n",
    "                                                                              hours=hour,\n",
    "                                                                              minutes=minute,\n",
    "                                                                              seconds=second)\n",
    "    return dt_obj\n",
    "\n",
    "# ---------------------------\n",
    "# S3 downloader / file selector\n",
    "# ---------------------------\n",
    "\n",
    "def file_time_from_dataset_minimal(fs, path):\n",
    "    \"\"\"\n",
    "    Try to read a time from dataset attributes or coords without loading all data.\n",
    "    Returns timezone-aware datetime or None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with fs.open(path, mode='rb') as f:\n",
    "            ds = xr.open_dataset(f, decode_times=False)\n",
    "            # Prefer a time coord if present\n",
    "            if 'time' in ds.coords:\n",
    "                try:\n",
    "                    t = xr.decode_cf(ds[['time']])['time'].values\n",
    "                    if hasattr(t, '__len__'):\n",
    "                        t = t[0]\n",
    "                    # Convert numpy datetime64 to python datetime (UTC)\n",
    "                    py_dt = pd.to_datetime(t).to_pydatetime()\n",
    "                    if py_dt.tzinfo is None:\n",
    "                        py_dt = py_dt.replace(tzinfo=dt.timezone.utc)\n",
    "                    return py_dt\n",
    "                except Exception:\n",
    "                    pass\n",
    "            # try global attributes commonly present\n",
    "            for attr in ('time_coverage_start', 'Time_coverage_start', 'start_time'):\n",
    "                ts = getattr(ds, attr, None)\n",
    "                if ts:\n",
    "                    try:\n",
    "                        py_dt = dt.datetime.fromisoformat(ts.replace('Z', '+00:00'))\n",
    "                        return py_dt\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            # Fallback: parse from filename\n",
    "            fname = path.split('/')[-1]\n",
    "            return extract_goes_time_from_filename(fname)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def get_peak_milton_data(year, month, day, hour, minute=0):\n",
    "    \"\"\"\n",
    "    Find and load the GOES file closest to target UTC time.\n",
    "    Returns xarray.Dataset or None.\n",
    "    \"\"\"\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "    jday = dt.datetime(year, month, day).timetuple().tm_yday\n",
    "    syr = f\"{year:04d}\"\n",
    "    sjd = f\"{jday:03d}\"\n",
    "    shr = f\"{hour:02d}\"\n",
    "\n",
    "    pattern = f\"s3://{S3_BUCKET}/{PRODUCT}/{syr}/{sjd}/{shr}/*{BAND_TOKEN}*.nc\"\n",
    "    files = fs.glob(pattern)\n",
    "    if not files:\n",
    "        print(f\"  ‚ö†Ô∏è No files for {hour:02d}:{minute:02d}Z (pattern: {pattern})\")\n",
    "        return None\n",
    "\n",
    "    target_dt = dt.datetime(year, month, day, hour, minute, tzinfo=dt.timezone.utc)\n",
    "\n",
    "    candidates = []\n",
    "    for p in files:\n",
    "        fname = p.rsplit('/', 1)[-1]\n",
    "        file_dt = extract_goes_time_from_filename(fname)\n",
    "        if file_dt is None:\n",
    "            # try reading minimal metadata (slower)\n",
    "            file_dt = file_time_from_dataset_minimal(fs, p)\n",
    "        if file_dt is None:\n",
    "            continue\n",
    "        diff_min = abs((file_dt - target_dt).total_seconds()) / 60.0\n",
    "        candidates.append({'path': p, 'fname': fname, 'time': file_dt, 'diff': diff_min})\n",
    "\n",
    "    if not candidates:\n",
    "        print(\"  ‚ö†Ô∏è No files with readable timestamps\")\n",
    "        return None\n",
    "\n",
    "    best = min(candidates, key=lambda x: x['diff'])\n",
    "    print(f\"  üì° Target: {hour:02d}:{minute:02d}Z  ->  Actual: {best['time'].strftime('%Y-%m-%dT%H:%M:%SZ')}  ({best['fname']})\")\n",
    "\n",
    "    try:\n",
    "        with fs.open(best['path'], mode='rb') as f:\n",
    "            ds = xr.open_dataset(f)\n",
    "            ds.load()  # load into memory; consider lazy if memory is a concern\n",
    "        return ds\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "# ---------------------------\n",
    "# Pure-pyproj GOES projection (auto-detect)\n",
    "# ---------------------------\n",
    "\n",
    "def convert_goes_coordinates_pyproj(ds):\n",
    "    \"\"\"\n",
    "    Pure-pyproj projection converting GOES ABI x/y -> lat/lon.\n",
    "    Auto-detects whether x/y are in radians or meters and handles conversion.\n",
    "\n",
    "    Returns: lat_2d, lon_2d, data (2D)\n",
    "    \"\"\"\n",
    "    # Basic checks\n",
    "    if 'CMI' not in ds:\n",
    "        raise RuntimeError(\"Dataset does not contain 'CMI' variable\")\n",
    "\n",
    "    if 'goes_imager_projection' not in ds:\n",
    "        raise RuntimeError(\"Dataset missing 'goes_imager_projection' variable (required attributes)\")\n",
    "\n",
    "    proj_attrs = ds['goes_imager_projection'].attrs\n",
    "\n",
    "    # Extract projection attributes with safe fallbacks\n",
    "    # CF names: perspective_point_height, semi_major_axis, semi_minor_axis, longitude_of_projection_origin, sweep_angle_axis\n",
    "    try:\n",
    "        perspective_point_height = float(proj_attrs.get('perspective_point_height', proj_attrs.get('h', None)))\n",
    "    except Exception:\n",
    "        perspective_point_height = None\n",
    "\n",
    "    try:\n",
    "        semi_major_axis = float(proj_attrs.get('semi_major_axis', proj_attrs.get('semi_major', 6378137.0)))\n",
    "    except Exception:\n",
    "        semi_major_axis = 6378137.0\n",
    "\n",
    "    try:\n",
    "        semi_minor_axis = float(proj_attrs.get('semi_minor_axis', proj_attrs.get('semi_minor', semi_major_axis)))\n",
    "    except Exception:\n",
    "        semi_minor_axis = semi_major_axis\n",
    "\n",
    "    lon_0 = float(proj_attrs.get('longitude_of_projection_origin', proj_attrs.get('longitude_of_projection', 0.0)))\n",
    "    sweep = proj_attrs.get('sweep_angle_axis', proj_attrs.get('sweep', 'x'))\n",
    "    # normalize sweep to 'x' or 'y'\n",
    "    if isinstance(sweep, bytes):\n",
    "        sweep = sweep.decode('utf-8')\n",
    "    sweep = sweep if sweep in ('x', 'y') else 'x'\n",
    "\n",
    "    # Read x/y arrays\n",
    "    if 'x' not in ds.coords or 'y' not in ds.coords:\n",
    "        # try variables\n",
    "        if 'x' in ds and 'y' in ds:\n",
    "            x = ds['x'].values\n",
    "            y = ds['y'].values\n",
    "        else:\n",
    "            raise RuntimeError(\"Dataset missing x/y coordinate arrays\")\n",
    "    else:\n",
    "        x = ds['x'].values\n",
    "        y = ds['y'].values\n",
    "\n",
    "    # Auto-detect units: if absolute max < 1.5 -> assume radians; else meters\n",
    "    max_abs_x = np.nanmax(np.abs(x))\n",
    "    max_abs_y = np.nanmax(np.abs(y))\n",
    "    is_radians = (max_abs_x < 1.5) and (max_abs_y < 1.5)\n",
    "\n",
    "    if is_radians:\n",
    "        if perspective_point_height is None:\n",
    "            print(\"  ‚ö†Ô∏è x/y appear to be in radians but perspective_point_height is missing; attempting to continue using semi-major axis fallback.\")\n",
    "            # fallback to a large value; this is not ideal but gives something\n",
    "            H = semi_major_axis * 5.0\n",
    "        else:\n",
    "            H = perspective_point_height\n",
    "        # Convert radians -> meters by multiplying by H (common approach)\n",
    "        x_m = x * H\n",
    "        y_m = y * H\n",
    "    else:\n",
    "        x_m = x\n",
    "        y_m = y\n",
    "        H = perspective_point_height if perspective_point_height is not None else semi_major_axis * 5.0\n",
    "\n",
    "    # Prepare the PROJ geos projection\n",
    "    # Note: pyproj.Proj accepts proj='geos' with params h, lon_0, a, b, sweep\n",
    "    try:\n",
    "        geos_proj = Proj(proj='geos',\n",
    "                         h=perspective_point_height if perspective_point_height is not None else H,\n",
    "                         lon_0=lon_0,\n",
    "                         sweep=sweep,\n",
    "                         a=semi_major_axis,\n",
    "                         b=semi_minor_axis,\n",
    "                         x_0=0, y_0=0,\n",
    "                         units='m')\n",
    "    except Exception as e:\n",
    "        # fallback to proj string\n",
    "        proj_str = f\"+proj=geos +h={perspective_point_height or H} +lon_0={lon_0} +sweep={sweep} +a={semi_major_axis} +b={semi_minor_axis} +units=m +no_defs\"\n",
    "        geos_proj = Proj(proj_str)\n",
    "\n",
    "    # Transformer to WGS84 (lon/lat)\n",
    "    transformer = Transformer.from_proj(geos_proj, CRS.from_epsg(4326), always_xy=True)\n",
    "\n",
    "    # Build meshgrid (note x correspond to columns, y to rows)\n",
    "    XX, YY = np.meshgrid(x_m, y_m)\n",
    "\n",
    "    # perform transform; transformer.transform can accept ndarray\n",
    "    lon2d, lat2d = transformer.transform(XX, YY)\n",
    "\n",
    "    return lat2d, lon2d, ds['CMI'].values\n",
    "\n",
    "# ---------------------------\n",
    "# Frame processing\n",
    "# ---------------------------\n",
    "\n",
    "def process_peak_frame(frame_num, hour, minute):\n",
    "    print(f\"\\n[{frame_num:02d}] Processing {hour:02d}:{minute:02d} UTC...\")\n",
    "    ds = get_peak_milton_data(PEAK_YEAR, PEAK_MONTH, PEAK_DAY, hour, minute)\n",
    "    if ds is None:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        lat_grid, lon_grid, cmi = convert_goes_coordinates_pyproj(ds)\n",
    "        temp_c = cmi - 273.15\n",
    "        print(f\"  ‚úÖ Projection successful: lat {lat_grid.shape}, lon {lon_grid.shape}, data {cmi.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Projection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Mask valid data\n",
    "    valid_mask = ~np.isnan(temp_c) & np.isfinite(lon_grid) & np.isfinite(lat_grid)\n",
    "    if not np.any(valid_mask):\n",
    "        print(\"  ‚ùå No valid data after masking\")\n",
    "        return None\n",
    "\n",
    "    lon_flat = lon_grid[valid_mask]\n",
    "    lat_flat = lat_grid[valid_mask]\n",
    "    temp_flat = temp_c[valid_mask]\n",
    "\n",
    "    print(f\"  üìä Total valid points: {len(lon_flat):,}\")\n",
    "\n",
    "    # Filter to Gulf region where Milton was\n",
    "    gulf_mask = (lon_flat > -95) & (lon_flat < -75) & (lat_flat > 20) & (lat_flat < 32)\n",
    "\n",
    "    if np.any(gulf_mask):\n",
    "        lon_flat = lon_flat[gulf_mask]\n",
    "        lat_flat = lat_flat[gulf_mask]\n",
    "        temp_flat = temp_flat[gulf_mask]\n",
    "        print(f\"  üåä Gulf region points: {len(lon_flat):,}\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è No points in Gulf, using all points\")\n",
    "\n",
    "    if len(lon_flat) == 0:\n",
    "        print(\"  ‚ùå No data after filtering\")\n",
    "        return None\n",
    "\n",
    "    # Downsample for D3 / export\n",
    "    D3_MAX_POINTS = 2000\n",
    "    if len(lon_flat) > D3_MAX_POINTS:\n",
    "        step = max(1, len(lon_flat) // D3_MAX_POINTS)\n",
    "        idx = np.arange(0, len(lon_flat), step)\n",
    "        lon_flat = lon_flat[idx]\n",
    "        lat_flat = lat_flat[idx]\n",
    "        temp_flat = temp_flat[idx]\n",
    "        print(f\"  ‚¨áÔ∏è Downsampled to: {len(lon_flat):,} points\")\n",
    "\n",
    "    min_temp = np.nanmin(temp_flat)\n",
    "\n",
    "    # Category mapping (based on min brightness temp C)\n",
    "    if min_temp < -82:\n",
    "        category, wind_mph = 5, 160\n",
    "    elif min_temp < -77:\n",
    "        category, wind_mph = 4, 140\n",
    "    elif min_temp < -72:\n",
    "        category, wind_mph = 3, 125\n",
    "    elif min_temp < -67:\n",
    "        category, wind_mph = 2, 105\n",
    "    elif min_temp < -62:\n",
    "        category, wind_mph = 1, 90\n",
    "    else:\n",
    "        category, wind_mph = 0, 75\n",
    "\n",
    "    # Estimate center from coldest pixels\n",
    "    cold_mask = temp_flat < -50\n",
    "    if np.any(cold_mask):\n",
    "        center_lon = float(np.mean(lon_flat[cold_mask]))\n",
    "        center_lat = float(np.mean(lat_flat[cold_mask]))\n",
    "    else:\n",
    "        hours_from_12 = (hour + minute / 60.0) - 12.0\n",
    "        center_lon = -86.0 + hours_from_12 * 0.67\n",
    "        center_lat = 24.5 + hours_from_12 * 0.17\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'lon': lon_flat,\n",
    "        'lat': lat_flat,\n",
    "        'temp_c': temp_flat,\n",
    "        'temp_k': temp_flat + 273.15\n",
    "    })\n",
    "\n",
    "    df['frame'] = frame_num\n",
    "    df['hour'] = hour\n",
    "    df['minute'] = minute\n",
    "    df['time_str'] = f\"{hour:02d}:{minute:02d}\"\n",
    "    df['category'] = int(category)\n",
    "    df['wind_mph'] = int(wind_mph)\n",
    "    df['center_lon'] = center_lon\n",
    "    df['center_lat'] = center_lat\n",
    "    df['min_temp_c'] = float(min_temp)\n",
    "\n",
    "    filename = f\"milton_cat5_{frame_num:02d}.csv\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    df.to_csv(filepath, index=False)\n",
    "\n",
    "    print(f\"  ‚úÖ Saved to {filename}\")\n",
    "    print(f\"  üå°Ô∏è Min temp: {min_temp:.1f}¬∞C\")\n",
    "    print(f\"  üåÄ Category: {category}\")\n",
    "    print(f\"  üí® Wind: ~{wind_mph} mph\")\n",
    "    print(f\"  üìç Center: {center_lon:.3f}¬∞W, {center_lat:.3f}¬∞N\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---------------------------\n",
    "# Main: test one frame then process all frames\n",
    "# ---------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43a1409e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTING PROJECTION WITH ONE FRAME...\n",
      "======================================================================\n",
      "\n",
      "[00] Processing 12:00 UTC...\n",
      "  üì° Target: 12:00Z  ->  Actual: 2024-10-08T12:01:17Z  (OR_ABI-L2-CMIPC-M6C13_G16_s20242821201173_e20242821203558_c20242821204045.nc)\n",
      "  ‚úÖ Projection successful: lat (1500, 2500), lon (1500, 2500), data (1500, 2500)\n",
      "  üìä Total valid points: 3,702,838\n",
      "  üåä Gulf region points: 528,522\n",
      "  ‚¨áÔ∏è Downsampled to: 2,002 points\n",
      "  ‚úÖ Saved to milton_cat5_00.csv\n",
      "  üå°Ô∏è Min temp: -81.7¬∞C\n",
      "  üåÄ Category: 4\n",
      "  üí® Wind: ~140 mph\n",
      "  üìç Center: -85.194¬∞W, 23.448¬∞N\n",
      "\n",
      "‚úÖ TEST SUCCESSFUL! Processing all frames...\n",
      "   Sample data shape: (2002, 13)\n",
      "   Columns: ['lon', 'lat', 'temp_c', 'temp_k', 'frame', 'hour', 'minute', 'time_str', 'category', 'wind_mph', 'center_lon', 'center_lat', 'min_temp_c']\n",
      "\n",
      "Processing 13 frames...\n",
      "\n",
      "[01] Processing 12:30 UTC...\n",
      "  üì° Target: 12:30Z  ->  Actual: 2024-10-08T12:31:17Z  (OR_ABI-L2-CMIPC-M6C13_G16_s20242821231173_e20242821233558_c20242821234058.nc)\n",
      "  ‚úÖ Projection successful: lat (1500, 2500), lon (1500, 2500), data (1500, 2500)\n",
      "  üìä Total valid points: 3,702,838\n",
      "  üåä Gulf region points: 528,522\n",
      "  ‚¨áÔ∏è Downsampled to: 2,002 points\n",
      "  ‚úÖ Saved to milton_cat5_01.csv\n",
      "  üå°Ô∏è Min temp: -80.4¬∞C\n",
      "  üåÄ Category: 4\n",
      "  üí® Wind: ~140 mph\n",
      "  üìç Center: -85.502¬∞W, 23.389¬∞N\n",
      "\n",
      "[02] Processing 13:00 UTC...\n",
      "  üì° Target: 13:00Z  ->  Actual: 2024-10-08T13:01:17Z  (OR_ABI-L2-CMIPC-M6C13_G16_s20242821301173_e20242821303558_c20242821304063.nc)\n",
      "  ‚úÖ Projection successful: lat (1500, 2500), lon (1500, 2500), data (1500, 2500)\n",
      "  üìä Total valid points: 3,702,838\n",
      "  üåä Gulf region points: 528,522\n",
      "  ‚¨áÔ∏è Downsampled to: 2,002 points\n",
      "  ‚úÖ Saved to milton_cat5_02.csv\n",
      "  üå°Ô∏è Min temp: -80.3¬∞C\n",
      "  üåÄ Category: 4\n",
      "  üí® Wind: ~140 mph\n",
      "  üìç Center: -85.281¬∞W, 23.441¬∞N\n",
      "\n",
      "[03] Processing 13:30 UTC...\n",
      "  üì° Target: 13:30Z  ->  Actual: 2024-10-08T13:31:17Z  (OR_ABI-L2-CMIPC-M6C13_G16_s20242821331173_e20242821333558_c20242821334064.nc)\n",
      "  ‚úÖ Projection successful: lat (1500, 2500), lon (1500, 2500), data (1500, 2500)\n",
      "  üìä Total valid points: 3,702,838\n",
      "  üåä Gulf region points: 528,522\n",
      "  ‚¨áÔ∏è Downsampled to: 2,002 points\n",
      "  ‚úÖ Saved to milton_cat5_03.csv\n",
      "  üå°Ô∏è Min temp: -79.8¬∞C\n",
      "  üåÄ Category: 4\n",
      "  üí® Wind: ~140 mph\n",
      "  üìç Center: -85.302¬∞W, 23.339¬∞N\n",
      "\n",
      "[04] Processing 14:00 UTC...\n",
      "  üì° Target: 14:00Z  ->  Actual: 2024-10-08T14:01:17Z  (OR_ABI-L2-CMIPC-M6C13_G16_s20242821401173_e20242821403558_c20242821404063.nc)\n",
      "  ‚úÖ Projection successful: lat (1500, 2500), lon (1500, 2500), data (1500, 2500)\n",
      "  üìä Total valid points: 3,702,838\n",
      "  üåä Gulf region points: 528,522\n",
      "  ‚¨áÔ∏è Downsampled to: 2,002 points\n",
      "  ‚úÖ Saved to milton_cat5_04.csv\n",
      "  üå°Ô∏è Min temp: -79.7¬∞C\n",
      "  üåÄ Category: 4\n",
      "  üí® Wind: ~140 mph\n",
      "  üìç Center: -85.350¬∞W, 23.418¬∞N\n",
      "\n",
      "[05] Processing 14:30 UTC...\n",
      "  üì° Target: 14:30Z  ->  Actual: 2024-10-08T14:31:17Z  (OR_ABI-L2-CMIPC-M6C13_G16_s20242821431173_e20242821433558_c20242821434054.nc)\n",
      "  ‚úÖ Projection successful: lat (1500, 2500), lon (1500, 2500), data (1500, 2500)\n",
      "  üìä Total valid points: 3,702,838\n",
      "  üåä Gulf region points: 528,522\n",
      "  ‚¨áÔ∏è Downsampled to: 2,002 points\n",
      "  ‚úÖ Saved to milton_cat5_05.csv\n",
      "  üå°Ô∏è Min temp: -80.4¬∞C\n",
      "  üåÄ Category: 4\n",
      "  üí® Wind: ~140 mph\n",
      "  üìç Center: -85.350¬∞W, 23.532¬∞N\n",
      "\n",
      "[06] Processing 15:00 UTC...\n",
      "  üì° Target: 15:00Z  ->  Actual: 2024-10-08T15:01:17Z  (OR_ABI-L2-CMIPC-M6C13_G16_s20242821501174_e20242821503558_c20242821504069.nc)\n",
      "  ‚úÖ Projection successful: lat (1500, 2500), lon (1500, 2500), data (1500, 2500)\n",
      "  üìä Total valid points: 3,702,838\n",
      "  üåä Gulf region points: 528,522\n",
      "  ‚¨áÔ∏è Downsampled to: 2,002 points\n",
      "  ‚úÖ Saved to milton_cat5_06.csv\n",
      "  üå°Ô∏è Min temp: -80.4¬∞C\n",
      "  üåÄ Category: 4\n",
      "  üí® Wind: ~140 mph\n",
      "  üìç Center: -85.714¬∞W, 23.468¬∞N\n",
      "\n",
      "[07] Processing 15:30 UTC...\n",
      "  üì° Target: 15:30Z  ->  Actual: 2024-10-08T15:31:17Z  (OR_ABI-L2-CMIPC-M6C13_G16_s20242821531174_e20242821533559_c20242821534048.nc)\n",
      "  ‚úÖ Projection successful: lat (1500, 2500), lon (1500, 2500), data (1500, 2500)\n",
      "  üìä Total valid points: 3,702,838\n",
      "  üåä Gulf region points: 528,522\n",
      "  ‚¨áÔ∏è Downsampled to: 2,002 points\n",
      "  ‚úÖ Saved to milton_cat5_07.csv\n",
      "  üå°Ô∏è Min temp: -81.2¬∞C\n",
      "  üåÄ Category: 4\n",
      "  üí® Wind: ~140 mph\n",
      "  üìç Center: -85.803¬∞W, 23.506¬∞N\n",
      "\n",
      "[08] Processing 16:00 UTC...\n",
      "  üì° Target: 16:00Z  ->  Actual: 2024-10-08T16:01:17Z  (OR_ABI-L2-CMIPC-M6C13_G16_s20242821601174_e20242821603558_c20242821604067.nc)\n",
      "  ‚úÖ Projection successful: lat (1500, 2500), lon (1500, 2500), data (1500, 2500)\n",
      "  üìä Total valid points: 3,702,838\n",
      "  üåä Gulf region points: 528,522\n",
      "  ‚¨áÔ∏è Downsampled to: 2,002 points\n",
      "  ‚úÖ Saved to milton_cat5_08.csv\n",
      "  üå°Ô∏è Min temp: -80.3¬∞C\n",
      "  üåÄ Category: 4\n",
      "  üí® Wind: ~140 mph\n",
      "  üìç Center: -85.767¬∞W, 23.455¬∞N\n",
      "\n",
      "[09] Processing 16:30 UTC...\n",
      "  üì° Target: 16:30Z  ->  Actual: 2024-10-08T16:31:17Z  (OR_ABI-L2-CMIPC-M6C13_G16_s20242821631174_e20242821633558_c20242821634061.nc)\n",
      "  ‚úÖ Projection successful: lat (1500, 2500), lon (1500, 2500), data (1500, 2500)\n",
      "  üìä Total valid points: 3,702,838\n",
      "  üåä Gulf region points: 528,522\n",
      "  ‚¨áÔ∏è Downsampled to: 2,002 points\n",
      "  ‚úÖ Saved to milton_cat5_09.csv\n",
      "  üå°Ô∏è Min temp: -79.8¬∞C\n",
      "  üåÄ Category: 4\n",
      "  üí® Wind: ~140 mph\n",
      "  üìç Center: -85.830¬∞W, 23.574¬∞N\n",
      "\n",
      "[10] Processing 17:00 UTC...\n",
      "  üì° Target: 17:00Z  ->  Actual: 2024-10-08T17:01:17Z  (OR_ABI-L2-CMIPC-M6C13_G16_s20242821701174_e20242821703558_c20242821704070.nc)\n",
      "  ‚úÖ Projection successful: lat (1500, 2500), lon (1500, 2500), data (1500, 2500)\n",
      "  üìä Total valid points: 3,702,838\n",
      "  üåä Gulf region points: 528,522\n",
      "  ‚¨áÔ∏è Downsampled to: 2,002 points\n",
      "  ‚úÖ Saved to milton_cat5_10.csv\n",
      "  üå°Ô∏è Min temp: -79.5¬∞C\n",
      "  üåÄ Category: 4\n",
      "  üí® Wind: ~140 mph\n",
      "  üìç Center: -85.889¬∞W, 23.680¬∞N\n",
      "\n",
      "[11] Processing 17:30 UTC...\n",
      "  üì° Target: 17:30Z  ->  Actual: 2024-10-08T17:31:17Z  (OR_ABI-L2-CMIPC-M6C13_G16_s20242821731174_e20242821733558_c20242821734073.nc)\n",
      "  ‚úÖ Projection successful: lat (1500, 2500), lon (1500, 2500), data (1500, 2500)\n",
      "  üìä Total valid points: 3,702,838\n",
      "  üåä Gulf region points: 528,522\n",
      "  ‚¨áÔ∏è Downsampled to: 2,002 points\n",
      "  ‚úÖ Saved to milton_cat5_11.csv\n",
      "  üå°Ô∏è Min temp: -83.3¬∞C\n",
      "  üåÄ Category: 5\n",
      "  üí® Wind: ~160 mph\n",
      "  üìç Center: -85.751¬∞W, 23.776¬∞N\n",
      "\n",
      "[12] Processing 18:00 UTC...\n",
      "  üì° Target: 18:00Z  ->  Actual: 2024-10-08T18:01:17Z  (OR_ABI-L2-CMIPC-M6C13_G16_s20242821801174_e20242821803559_c20242821804061.nc)\n",
      "  ‚úÖ Projection successful: lat (1500, 2500), lon (1500, 2500), data (1500, 2500)\n",
      "  üìä Total valid points: 3,702,838\n",
      "  üåä Gulf region points: 528,522\n",
      "  ‚¨áÔ∏è Downsampled to: 2,002 points\n",
      "  ‚úÖ Saved to milton_cat5_12.csv\n",
      "  üå°Ô∏è Min temp: -80.7¬∞C\n",
      "  üåÄ Category: 4\n",
      "  üí® Wind: ~140 mph\n",
      "  üìç Center: -85.915¬∞W, 23.740¬∞N\n",
      "\n",
      "======================================================================\n",
      "PROCESSING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "‚úÖ SUCCESS: Created 13 frames\n",
      "üìÅ Output: milton_peak_cat5/\n",
      "üìã Timeline: milton_peak_cat5\\timeline.json\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TESTING PROJECTION WITH ONE FRAME...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_df = process_peak_frame(0, 12, 0)\n",
    "\n",
    "if test_df is not None:\n",
    "    print(f\"\\n‚úÖ TEST SUCCESSFUL! Processing all frames...\")\n",
    "    print(f\"   Sample data shape: {test_df.shape}\")\n",
    "    print(f\"   Columns: {test_df.columns.tolist()}\")\n",
    "\n",
    "    # Build frames list\n",
    "    frames = []\n",
    "    frame_num = 0\n",
    "    for hour in range(PEAK_START_HOUR, PEAK_END_HOUR + 1):\n",
    "        for minute in (0, 30):\n",
    "            # skip half-hour beyond end hour\n",
    "            if hour == PEAK_END_HOUR and minute == 30:\n",
    "                continue\n",
    "            frames.append({'frame_num': frame_num, 'hour': hour, 'minute': minute})\n",
    "            frame_num += 1\n",
    "\n",
    "    print(f\"\\nProcessing {len(frames)} frames...\")\n",
    "\n",
    "    processed = []\n",
    "    for f in frames:\n",
    "        if f['frame_num'] == 0:\n",
    "            processed.append(test_df)\n",
    "            continue\n",
    "        df = process_peak_frame(f['frame_num'], f['hour'], f['minute'])\n",
    "        if df is not None:\n",
    "            processed.append(df)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PROCESSING COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    if processed:\n",
    "        # Save timeline\n",
    "        summary = {\n",
    "            \"hurricane\": \"Milton\",\n",
    "            \"year\": PEAK_YEAR,\n",
    "            \"peak_category\": 5,\n",
    "            \"total_frames\": len(processed),\n",
    "            \"interval_minutes\": INTERVAL_MINUTES,\n",
    "            \"frames\": []\n",
    "        }\n",
    "\n",
    "        for f in frames:\n",
    "            filename = f\"milton_cat5_{f['frame_num']:02d}.csv\"\n",
    "            filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "            if os.path.exists(filepath):\n",
    "                try:\n",
    "                    row = pd.read_csv(filepath, nrows=1)\n",
    "                    summary[\"frames\"].append({\n",
    "                        \"frame\": f['frame_num'],\n",
    "                        \"hour\": f['hour'],\n",
    "                        \"minute\": f['minute'],\n",
    "                        \"filename\": filename,\n",
    "                        \"time_str\": f\"{f['hour']:02d}:{f['minute']:02d}\",\n",
    "                        \"category\": int(row['category'].iloc[0]),\n",
    "                        \"wind_mph\": int(row['wind_mph'].iloc[0]),\n",
    "                        \"center_lon\": float(row['center_lon'].iloc[0]),\n",
    "                        \"center_lat\": float(row['center_lat'].iloc[0])\n",
    "                    })\n",
    "                except Exception:\n",
    "                    summary[\"frames\"].append({\n",
    "                        \"frame\": f['frame_num'],\n",
    "                        \"hour\": f['hour'],\n",
    "                        \"minute\": f['minute'],\n",
    "                        \"filename\": filename,\n",
    "                        \"time_str\": f\"{f['hour']:02d}:{f['minute']:02d}\",\n",
    "                        \"category\": 5,\n",
    "                        \"wind_mph\": 160\n",
    "                    })\n",
    "        timeline_path = os.path.join(OUTPUT_DIR, \"timeline.json\")\n",
    "        with open(timeline_path, 'w') as fh:\n",
    "            json.dump(summary, fh, indent=2)\n",
    "\n",
    "        print(f\"\\n‚úÖ SUCCESS: Created {len(processed)} frames\")\n",
    "        print(f\"üìÅ Output: {OUTPUT_DIR}/\")\n",
    "        print(f\"üìã Timeline: {timeline_path}\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå No frames processed\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Test failed. No output produced.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ae028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc106",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
