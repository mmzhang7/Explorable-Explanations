{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching file list for year 1975...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1975_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 1975...\n",
      "Fetching file list for year 1976...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1976_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 1976...\n",
      "Fetching file list for year 1977...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1977_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 1977...\n",
      "Fetching file list for year 1978...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1978_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 1978...\n",
      "Fetching file list for year 1979...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1979_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 1979...\n",
      "Fetching file list for year 1980...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1980_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 1980...\n",
      "Fetching file list for year 1981...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1981_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 1981...\n",
      "Fetching file list for year 1982...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1982_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 1982...\n",
      "Fetching file list for year 1983...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1983_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 1983...\n",
      "Fetching file list for year 1984...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1984_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 1984...\n",
      "Fetching file list for year 1985...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1985_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 1985...\n",
      "Fetching file list for year 1986...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1986_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 1986...\n",
      "Fetching file list for year 1987...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1987_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 1987...\n",
      "Fetching file list for year 1988...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1988_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 1988...\n",
      "Fetching file list for year 1989...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1989_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 1989...\n",
      "Fetching file list for year 1990...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1990_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 1990...\n",
      "Fetching file list for year 1991...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1991_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 1991...\n",
      "Fetching file list for year 1992...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1992_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 1992...\n",
      "Fetching file list for year 1993...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1993_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 1993...\n",
      "Fetching file list for year 1994...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1994_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 1994...\n",
      "Fetching file list for year 1995...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1995_c20250520.csv.gz...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/7dqqxm1j2wzd6plwtdt273k00000gn/T/ipykernel_27576/2327476676.py:59: DtypeWarning: Columns (26,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering hurricane/tropical storm events for 1995...\n",
      "Fetching file list for year 1996...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1996_c20250520.csv.gz...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/7dqqxm1j2wzd6plwtdt273k00000gn/T/ipykernel_27576/2327476676.py:59: DtypeWarning: Columns (26,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering hurricane/tropical storm events for 1996...\n",
      "Fetching file list for year 1997...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1997_c20250520.csv.gz...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/7dqqxm1j2wzd6plwtdt273k00000gn/T/ipykernel_27576/2327476676.py:59: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering hurricane/tropical storm events for 1997...\n",
      "Fetching file list for year 1998...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1998_c20250520.csv.gz...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/7dqqxm1j2wzd6plwtdt273k00000gn/T/ipykernel_27576/2327476676.py:59: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering hurricane/tropical storm events for 1998...\n",
      "Fetching file list for year 1999...\n",
      "Downloading StormEvents_details-ftp_v1.0_d1999_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 1999...\n",
      "Fetching file list for year 2000...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2000_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2000...\n",
      "Fetching file list for year 2001...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2001_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2001...\n",
      "Fetching file list for year 2002...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2002_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2002...\n",
      "Fetching file list for year 2003...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2003_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2003...\n",
      "Fetching file list for year 2004...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2004_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2004...\n",
      "Fetching file list for year 2005...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2005_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2005...\n",
      "Fetching file list for year 2006...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2006_c20250520.csv.gz...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/7dqqxm1j2wzd6plwtdt273k00000gn/T/ipykernel_27576/2327476676.py:59: DtypeWarning: Columns (29,34,35,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering hurricane/tropical storm events for 2006...\n",
      "Fetching file list for year 2007...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2007_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2007...\n",
      "Fetching file list for year 2008...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2008_c20251204.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2008...\n",
      "Fetching file list for year 2009...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2009_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2009...\n",
      "Fetching file list for year 2010...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2010_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2010...\n",
      "Fetching file list for year 2011...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2011_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2011...\n",
      "Fetching file list for year 2012...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2012_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2012...\n",
      "Fetching file list for year 2013...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2013_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2013...\n",
      "Fetching file list for year 2014...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2014_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2014...\n",
      "Fetching file list for year 2015...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2015_c20251118.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2015...\n",
      "Fetching file list for year 2016...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2016_c20250818.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2016...\n",
      "Fetching file list for year 2017...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2017_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2017...\n",
      "Fetching file list for year 2018...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2018_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2018...\n",
      "Fetching file list for year 2019...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2019_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2019...\n",
      "Fetching file list for year 2020...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2020_c20251118.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2020...\n",
      "Fetching file list for year 2021...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2021_c20250520.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2021...\n",
      "Fetching file list for year 2022...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2022_c20250721.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2022...\n",
      "Fetching file list for year 2023...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2023_c20250731.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2023...\n",
      "Fetching file list for year 2024...\n",
      "Downloading StormEvents_details-ftp_v1.0_d2024_c20251204.csv.gz...\n",
      "Filtering hurricane/tropical storm events for 2024...\n",
      "Saving final dataset: hurricane_impacts_atlantic_states_1975_2024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/7dqqxm1j2wzd6plwtdt273k00000gn/T/ipykernel_27576/2327476676.py:77: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  result = pd.concat(all_events, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import gzip\n",
    "import io\n",
    "\n",
    "BASE_URL = \"https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/\"\n",
    "\n",
    "YEARS = list(range(1975, 2025))\n",
    "EVENT_TYPES = [\"Hurricane (Typhoon)\", \"Tropical Storm\"]\n",
    "\n",
    "# Full state names north/east of Texas\n",
    "ATLANTIC_GULF_STATES = {\n",
    "    \"LOUISIANA\", \"MISSISSIPPI\", \"ALABAMA\", \"FLORIDA\",\n",
    "    \"GEORGIA\", \"SOUTH CAROLINA\", \"NORTH CAROLINA\", \"VIRGINIA\",\n",
    "    \"MARYLAND\", \"DELAWARE\", \"NEW JERSEY\", \"PENNSYLVANIA\",\n",
    "    \"NEW YORK\", \"CONNECTICUT\", \"RHODE ISLAND\", \"MASSACHUSETTS\",\n",
    "    \"NEW HAMPSHIRE\", \"MAINE\"\n",
    "}\n",
    "\n",
    "def parse_damage(value):\n",
    "    \"\"\"Convert NOAA damage strings like '12.5K' or '1.2M' into numbers.\"\"\"\n",
    "    if pd.isna(value) or value == \"\":\n",
    "        return 0\n",
    "    value = value.strip()\n",
    "    if value[-1] in [\"K\", \"M\", \"B\"]:\n",
    "        multiplier = {\"K\": 1_000, \"M\": 1_000_000, \"B\": 1_000_000_000}[value[-1]]\n",
    "        try:\n",
    "            return float(value[:-1]) * multiplier\n",
    "        except:\n",
    "            return 0\n",
    "    else:\n",
    "        # Sometimes values appear as plain numbers\n",
    "        try:\n",
    "            return float(value)\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "all_events = []\n",
    "\n",
    "for year in YEARS:\n",
    "    print(f\"Fetching file list for year {year}...\")\n",
    "    index = requests.get(BASE_URL).text\n",
    "    \n",
    "    prefix = f\"StormEvents_details-ftp_v1.0_d{year}\"\n",
    "    files = [line.split('\"')[1] for line in index.splitlines() if prefix in line]\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"No details file found for year {year} â€” skipping.\")\n",
    "        continue\n",
    "    \n",
    "    filename = files[0]  # Take the newest version\n",
    "    url = BASE_URL + filename\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    \n",
    "    res = requests.get(url)\n",
    "    compressed = io.BytesIO(res.content)\n",
    "    \n",
    "    with gzip.open(compressed, \"rt\", encoding=\"latin1\") as f:\n",
    "        df = pd.read_csv(f)\n",
    "    \n",
    "    print(f\"Filtering hurricane/tropical storm events for {year}...\")\n",
    "    # Filter by event type\n",
    "    df = df[df[\"EVENT_TYPE\"].isin(EVENT_TYPES)]\n",
    "    \n",
    "    # Filter by states north/east of Texas\n",
    "    df = df[df[\"STATE\"].str.upper().isin(ATLANTIC_GULF_STATES)]\n",
    "    \n",
    "    # Parse damage fields\n",
    "    df[\"DAMAGE_PROPERTY_NUM\"] = df[\"DAMAGE_PROPERTY\"].apply(parse_damage)\n",
    "    df[\"DAMAGE_CROPS_NUM\"] = df[\"DAMAGE_CROPS\"].apply(parse_damage)\n",
    "    df[\"TOTAL_DAMAGE\"] = df[\"DAMAGE_PROPERTY_NUM\"] + df[\"DAMAGE_CROPS_NUM\"]\n",
    "    \n",
    "    all_events.append(df)\n",
    "\n",
    "# Combine all years\n",
    "if all_events:\n",
    "    result = pd.concat(all_events, ignore_index=True)\n",
    "    print(\"Saving final dataset: hurricane_impacts_atlantic_states_1975_2024.csv\")\n",
    "    result.to_csv(\"hurricane_impacts_atlantic_states_1975_2024.csv\", index=False)\n",
    "else:\n",
    "    print(\"No data collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CATEGORY  AVG_TOTAL_DAMAGE\n",
      "0         0      1.150751e+07\n",
      "1         1      4.739720e+07\n",
      "2         2      1.373194e+08\n",
      "3         3      3.470516e+08\n",
      "4         4      1.139130e+09\n",
      "5         5      6.000000e+09\n"
     ]
    }
   ],
   "source": [
    "# Make sure the CATEGORY column exists and fill missing values with 0 (tropical storms)\n",
    "if \"CATEGORY\" not in result.columns:\n",
    "    result[\"CATEGORY\"] = 0  # If the column is missing entirely\n",
    "else:\n",
    "    result[\"CATEGORY\"] = result[\"CATEGORY\"].fillna(0).astype(int)\n",
    "\n",
    "# Compute average total damage by CATEGORY\n",
    "avg_damage_by_category = (\n",
    "    result.groupby(\"CATEGORY\")[\"TOTAL_DAMAGE\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"TOTAL_DAMAGE\": \"AVG_TOTAL_DAMAGE\"})\n",
    ")\n",
    "\n",
    "print(avg_damage_by_category)\n",
    "\n",
    "# Optional: save to CSV\n",
    "avg_damage_by_category.to_csv(\"avg_damage_by_category.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
